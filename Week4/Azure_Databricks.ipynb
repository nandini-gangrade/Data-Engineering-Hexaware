{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgYdA4U7SrUI62v/HNsa4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 4: ETL Pipelines in Azure Databricks"
      ],
      "metadata": {
        "id": "2KFjbBXQewtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeETL\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "GFyw_psKCxyS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for customer data\n",
        "customer_data = [\n",
        "    (2001, 'Shruti', 'India'),\n",
        "    (2002, 'Priyanshi', 'Canada'),\n",
        "    (2003, 'Prakhar', 'New York'),\n",
        "    (2004, 'Shreya', 'USA'),\n",
        "    (2005, 'Divya', 'Australia')\n",
        "]\n",
        "\n",
        "customer_columns = [\"customer_id\", \"customer_name\", \"location\"]\n",
        "customer_df = spark.createDataFrame(customer_data, customer_columns)\n",
        "\n",
        "# Write the DataFrame to Delta format\n",
        "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/customer_dim\")\n",
        "\n",
        "# Create a DataFrame for order data\n",
        "order_data = [\n",
        "    (1001, 1, 2001, 1, 139.99, '2023-09-27 12:30:00'),\n",
        "    (1002, 2, 2002, 2, 934.99, '2023-09-27 14:45:00'),\n",
        "    (1003, 3, 2003, 1, 543.99, '2023-09-27 15:30:00'),\n",
        "    (1004, 1, 2004, 2, 299.99, '2023-09-27 16:00:00'),\n",
        "    (1005, 2, 2005, 1, 499.99, '2023-09-27 17:30:00')\n",
        "]\n",
        "\n",
        "order_columns = [\"order_id\", \"product_id\", \"customer_id\", \"quantity\", \"order_amount\", \"order_date\"]\n",
        "order_df = spark.createDataFrame(order_data, order_columns)\n",
        "\n",
        "# Write the order DataFrame to Delta format\n",
        "order_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/order_fact\")\n",
        "\n",
        "import dlt\n",
        "\n",
        "@dlt.table\n",
        "def order_fact():\n",
        "    return spark.read.format(\"delta\").load(\"/content/delta/order_fact\")\n",
        "\n",
        "@dlt.table\n",
        "def customer_dim():\n",
        "    return spark.read.format(\"delta\").load(\"/content/delta/customer_dim\")\n",
        "\n",
        "# Show data from Delta Live Tables\n",
        "df_dltCustomers = spark.read.format(\"delta\").load(\"/content/delta/customer_dim\")\n",
        "df_dltCustomers.show()\n",
        "\n",
        "df_dlt_Order_fact = spark.read.format(\"delta\").load(\"/content/delta/order_fact\")\n",
        "df_dlt_Order_fact.show()\n"
      ],
      "metadata": {
        "id": "1s-YzcisDaT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
