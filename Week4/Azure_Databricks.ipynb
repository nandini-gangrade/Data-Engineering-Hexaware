{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgYdA4U7SrUI62v/HNsa4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandini-gangrade/Data-Engineering-Hexaware/blob/Project/Week4/Azure_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1: Data Warehousing and SQL Concepts"
      ],
      "metadata": {
        "id": "2KFjbBXQewtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1JTrRidRR9b",
        "outputId": "cc7d1f18-16fe-4088-a18d-df69fce969bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Data:\n",
            "(2001, 'Bob', 'California')\n",
            "(2002, 'Alice', 'Texas')\n",
            "(2003, 'John', 'New York')\n",
            "\n",
            "Product Data:\n",
            "(1, 'Smartphone', 'Electronics')\n",
            "(2, 'Laptop', 'Electronics')\n",
            "(3, 'Tablet', 'Electronics')\n",
            "\n",
            "Order Data:\n",
            "(1001, 1, 2001, 1, 299.99, '2023-09-27 12:30:00')\n",
            "(1002, 2, 2002, 2, 999.99, '2023-09-27 14:45:00')\n",
            "(1003, 3, 2003, 1, 499.99, '2023-09-27 15:30:00')\n",
            "\n",
            "Total Sales per Product:\n",
            "('Laptop', 999.99)\n",
            "('Smartphone', 299.99)\n",
            "('Tablet', 499.99)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Set up SQLite in Colab\n",
        "import sqlite3\n",
        "\n",
        "# Create a new SQLite database (in-memory)\n",
        "conn = sqlite3.connect(':memory:')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Step 2: Create tables to store customer, product, and order data\n",
        "cursor.execute('''\n",
        "CREATE TABLE customer_dim (\n",
        "    customer_id INT PRIMARY KEY,\n",
        "    customer_name VARCHAR(255),\n",
        "    location VARCHAR(255)\n",
        ");\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE product_dim (\n",
        "    product_id INT PRIMARY KEY,\n",
        "    product_name VARCHAR(255),\n",
        "    category VARCHAR(255)\n",
        ");\n",
        "''')\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE order_fact (\n",
        "    order_id INT PRIMARY KEY,\n",
        "    product_id INT,\n",
        "    customer_id INT,\n",
        "    quantity INT,\n",
        "    order_amount DECIMAL(10, 2),\n",
        "    order_date TIMESTAMP\n",
        ");\n",
        "''')\n",
        "\n",
        "# Step 3: Insert sample data into the tables\n",
        "cursor.execute(\"INSERT INTO product_dim VALUES (1, 'Smartphone', 'Electronics')\")\n",
        "cursor.execute(\"INSERT INTO product_dim VALUES (2, 'Laptop', 'Electronics')\")\n",
        "cursor.execute(\"INSERT INTO product_dim VALUES (3, 'Tablet', 'Electronics')\")\n",
        "\n",
        "cursor.execute(\"INSERT INTO customer_dim VALUES (2001, 'Bob', 'California')\")\n",
        "cursor.execute(\"INSERT INTO customer_dim VALUES (2002, 'Alice', 'Texas')\")\n",
        "cursor.execute(\"INSERT INTO customer_dim VALUES (2003, 'John', 'New York')\")\n",
        "\n",
        "cursor.execute(\"INSERT INTO order_fact VALUES (1001, 1, 2001, 1, 299.99, '2023-09-27 12:30:00')\")\n",
        "cursor.execute(\"INSERT INTO order_fact VALUES (1002, 2, 2002, 2, 999.99, '2023-09-27 14:45:00')\")\n",
        "cursor.execute(\"INSERT INTO order_fact VALUES (1003, 3, 2003, 1, 499.99, '2023-09-27 15:30:00')\")\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "\n",
        "# Step 4: Query data to analyze customer behavior and product interactions\n",
        "# 1. Retrieve all customer data\n",
        "cursor.execute(\"SELECT * FROM customer_dim\")\n",
        "customers = cursor.fetchall()\n",
        "print(\"Customer Data:\")\n",
        "for row in customers:\n",
        "    print(row)\n",
        "\n",
        "# 2. Retrieve all product data\n",
        "cursor.execute(\"SELECT * FROM product_dim\")\n",
        "products = cursor.fetchall()\n",
        "print(\"\\nProduct Data:\")\n",
        "for row in products:\n",
        "    print(row)\n",
        "\n",
        "# 3. Retrieve all order data\n",
        "cursor.execute(\"SELECT * FROM order_fact\")\n",
        "orders = cursor.fetchall()\n",
        "print(\"\\nOrder Data:\")\n",
        "for row in orders:\n",
        "    print(row)\n",
        "\n",
        "# 4. Analyze total sales per product\n",
        "cursor.execute('''\n",
        "SELECT product_dim.product_name, SUM(order_fact.order_amount) AS total_sales\n",
        "FROM order_fact\n",
        "JOIN product_dim ON order_fact.product_id = product_dim.product_id\n",
        "GROUP BY product_dim.product_name\n",
        "''')\n",
        "sales_analysis = cursor.fetchall()\n",
        "print(\"\\nTotal Sales per Product:\")\n",
        "for row in sales_analysis:\n",
        "    print(row)\n",
        "\n",
        "# Close the connection\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2: Real-Time Data Processing with Python"
      ],
      "metadata": {
        "id": "BBXtlGRreo5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries in Colab\n",
        "!pip install sqlalchemy pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wsU5jOwRe4j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import random\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "metadata": {
        "id": "SNVMZ-zNfwrz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2.1: Create a connection to SQLite database\n",
        "# This creates a file 'ecommerce.db' in your Colab environment\n",
        "engine = create_engine('sqlite:///ecommerce.db')"
      ],
      "metadata": {
        "id": "px9NY-92gPx3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Function to simulate real-time data streams\n",
        "def simulate_order_stream():\n",
        "    product_ids = [1, 2, 3, 4]  # Product IDs available in the system\n",
        "    customer_ids = [2001, 2002, 2003]  # Customer IDs available in the system\n",
        "    batch_count = 0  # Initialize a counter for batch number\n",
        "    max_batches = 5  # Maximum number of batches to simulate\n",
        "\n",
        "    while batch_count < max_batches:\n",
        "        # Create a random order\n",
        "        order = {\n",
        "            'order_id': random.randint(1001, 9999),\n",
        "            'product_id': random.choice(product_ids),\n",
        "            'customer_id': random.choice(customer_ids),\n",
        "            'quantity': random.randint(1, 5),\n",
        "            'order_amount': round(random.uniform(50, 500), 2),\n",
        "            'order_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        # Yield the order as a Pandas DataFrame\n",
        "        yield pd.DataFrame([order])\n",
        "\n",
        "        # Increment the batch count\n",
        "        batch_count += 1\n",
        "\n",
        "        # Sleep for a short time to simulate real-time data\n",
        "        time.sleep(2)"
      ],
      "metadata": {
        "id": "zKDkV2dSgfV3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Stream and load real-time data into the SQL table\n",
        "# Run this cell to start the data streaming process\n",
        "for order_batch in simulate_order_stream():\n",
        "    # Append the generated order batch to the order_fact table\n",
        "    order_batch.to_sql('order_fact', con=engine, if_exists='append', index=False)\n",
        "\n",
        "    # Print only a summary of each order batch with the timestamp of loading\n",
        "    print(f\"Order batch loaded at {datetime.now()} - Order ID: {order_batch['order_id'].iloc[0]}, \"\n",
        "          f\"Product ID: {order_batch['product_id'].iloc[0]}, Customer ID: {order_batch['customer_id'].iloc[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6EBD7VRglEV",
        "outputId": "587d2ab3-6568-4b6a-90f0-d26c6cc8f8b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order batch loaded at 2024-09-27 08:20:14.097668 - Order ID: 2849, Product ID: 2, Customer ID: 2001\n",
            "Order batch loaded at 2024-09-27 08:20:16.113633 - Order ID: 6997, Product ID: 4, Customer ID: 2002\n",
            "Order batch loaded at 2024-09-27 08:20:18.131926 - Order ID: 8431, Product ID: 4, Customer ID: 2003\n",
            "Order batch loaded at 2024-09-27 08:20:20.148154 - Order ID: 1193, Product ID: 2, Customer ID: 2003\n",
            "Order batch loaded at 2024-09-27 08:20:22.164712 - Order ID: 2906, Product ID: 1, Customer ID: 2001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Week 3: Real-Time Data Processing with Apache Spark and PySpark"
      ],
      "metadata": {
        "id": "JPVFXzzhpQlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "# Set Up Spark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "firm8GrGpdEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
        "from pyspark.sql.functions import col, sum, count\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "e0Vbp2-bpqiv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CWqL-oOtE1Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EcommerceRealTime\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "pKXwPlw4p16j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a directory for streaming data\n",
        "streaming_data_path = os.makedirs('/content/streaming_data', exist_ok=True)\n",
        "\n",
        "# Step 4: Define schema for order data\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"order_amount\", FloatType(), True),\n",
        "    StructField(\"order_date\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Step 5: Simulate initial streaming data with new values\n",
        "initial_order_data = [\n",
        "    (2001, 1, 3001, 2, 149.99, datetime.now()),  # Order 1\n",
        "    (2002, 2, 3002, 3, 199.99, datetime.now()),  # Order 2\n",
        "    (2003, 3, 3003, 1, 299.99, datetime.now()),  # Order 3\n",
        "    (2004, 4, 3001, 4, 399.99, datetime.now()),  # Order 4\n",
        "    (2005, 5, 3002, 2, 599.99, datetime.now()),  # Order 5\n",
        "]\n",
        "\n",
        "# Create a DataFrame from the list of initial orders\n",
        "df_orders = spark.createDataFrame(initial_order_data, schema)\n",
        "\n",
        "# Show the initial data\n",
        "print(\"Initial Orders:\")\n",
        "df_orders.show()\n",
        "\n",
        "# Step 6: Process Data Using PySpark\n",
        "# Group the data by product_id and calculate the total sales (sum of order_amount) per product\n",
        "product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "print(\"Initial Product Sales:\")\n",
        "product_sales.show()\n",
        "\n",
        "# Step 7: Real-Time Streaming Simulation\n",
        "# Simulating appending new data in real-time with updated values\n",
        "new_order_data = [\n",
        "    (1008, 2, 3004, 2, 249.99, datetime.now()),  # New Order 1\n",
        "    (1009, 4, 3001, 3, 359.95, datetime.now()),  # New Order 2\n",
        "    (1010, 5, 3002, 1, 599.99, datetime.now()),  # New Order 3\n",
        "    (1011, 1, 3003, 4, 149.99, datetime.now()),  # New Order 4\n",
        "    (1012, 3, 3004, 2, 389.50, datetime.now()),  # New Order 5\n",
        "]\n",
        "\n",
        "# Create a new DataFrame for the new batch of orders with the correct timestamp format\n",
        "new_df_orders = spark.createDataFrame(new_order_data, schema)\n",
        "\n",
        "# Append the new data to the original DataFrame\n",
        "df_orders = df_orders.union(new_df_orders)\n",
        "\n",
        "# Perform the same aggregation again with the updated data\n",
        "updated_product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the updated result\n",
        "print(\"Updated Product Sales After New Orders:\")\n",
        "updated_product_sales.show()\n",
        "\n",
        "# Step 8: Write the results to a CSV file\n",
        "updated_product_sales.write.csv(\"/content/product_sales.csv\", header=True, mode='overwrite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOl5yp7ZqH-Y",
        "outputId": "fe59ce43-f96a-451d-dbc7-4c9694ddbf7f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Orders:\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "|order_id|product_id|customer_id|quantity|order_amount|          order_date|\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "|    2001|         1|       3001|       2|      149.99|2024-09-27 09:45:...|\n",
            "|    2002|         2|       3002|       3|      199.99|2024-09-27 09:45:...|\n",
            "|    2003|         3|       3003|       1|      299.99|2024-09-27 09:45:...|\n",
            "|    2004|         4|       3001|       4|      399.99|2024-09-27 09:45:...|\n",
            "|    2005|         5|       3002|       2|      599.99|2024-09-27 09:45:...|\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "\n",
            "Initial Product Sales:\n",
            "+----------+------------------+-----------+\n",
            "|product_id|       total_sales|order_count|\n",
            "+----------+------------------+-----------+\n",
            "|         1|149.99000549316406|          1|\n",
            "|         2|199.99000549316406|          1|\n",
            "|         3|  299.989990234375|          1|\n",
            "|         5|  599.989990234375|          1|\n",
            "|         4|  399.989990234375|          1|\n",
            "+----------+------------------+-----------+\n",
            "\n",
            "Updated Product Sales After New Orders:\n",
            "+----------+-----------------+-----------+\n",
            "|product_id|      total_sales|order_count|\n",
            "+----------+-----------------+-----------+\n",
            "|         1|299.9800109863281|          2|\n",
            "|         2|449.9800109863281|          2|\n",
            "|         3| 689.489990234375|          2|\n",
            "|         5| 1199.97998046875|          2|\n",
            "|         4|759.9400024414062|          2|\n",
            "+----------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Week 4: ETL Pipelines in Azure Databricks"
      ],
      "metadata": {
        "id": "H7IQOPnxBTzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install delta-spark"
      ],
      "metadata": {
        "id": "mM88y7R0BQxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeETL\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "GFyw_psKCxyS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for customer data\n",
        "customer_data = [\n",
        "    (2001, 'Shruti', 'India'),\n",
        "    (2002, 'Priyanshi', 'Canada'),\n",
        "    (2003, 'Prakhar', 'New York'),\n",
        "    (2004, 'Nandini', 'USA'),\n",
        "    (2005, 'Divya', 'Australia')\n",
        "]\n",
        "\n",
        "customer_columns = [\"customer_id\", \"customer_name\", \"location\"]\n",
        "customer_df = spark.createDataFrame(customer_data, customer_columns)\n",
        "\n",
        "# Write the DataFrame to Delta format\n",
        "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/customer_dim\")\n",
        "\n",
        "# Create a DataFrame for order data\n",
        "order_data = [\n",
        "    (1001, 1, 2001, 1, 139.99, '2023-09-27 12:30:00'),\n",
        "    (1002, 2, 2002, 2, 934.99, '2023-09-27 14:45:00'),\n",
        "    (1003, 3, 2003, 1, 543.99, '2023-09-27 15:30:00'),\n",
        "    (1004, 1, 2004, 2, 299.99, '2023-09-27 16:00:00'),\n",
        "    (1005, 2, 2005, 1, 499.99, '2023-09-27 17:30:00')\n",
        "]\n",
        "\n",
        "order_columns = [\"order_id\", \"product_id\", \"customer_id\", \"quantity\", \"order_amount\", \"order_date\"]\n",
        "order_df = spark.createDataFrame(order_data, order_columns)\n",
        "\n",
        "# Write the order DataFrame to Delta format\n",
        "order_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/delta/order_fact\")\n",
        "\n",
        "import dlt\n",
        "\n",
        "@dlt.table\n",
        "def order_fact():\n",
        "    return spark.read.format(\"delta\").load(\"/content/delta/order_fact\")\n",
        "\n",
        "@dlt.table\n",
        "def customer_dim():\n",
        "    return spark.read.format(\"delta\").load(\"/content/delta/customer_dim\")\n",
        "\n",
        "# Show data from Delta Live Tables\n",
        "df_dltCustomers = spark.read.format(\"delta\").load(\"/content/delta/customer_dim\")\n",
        "df_dltCustomers.show()\n",
        "\n",
        "df_dlt_Order_fact = spark.read.format(\"delta\").load(\"/content/delta/order_fact\")\n",
        "df_dlt_Order_fact.show()\n"
      ],
      "metadata": {
        "id": "1s-YzcisDaT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}