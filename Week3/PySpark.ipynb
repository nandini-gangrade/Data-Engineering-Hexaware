{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgMv1+4C3HcOnkjY/1QSwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nandini-gangrade/Data-Engineering-Hexaware/blob/Project/Week3/PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Week 3: Real-Time Data Processing with Apache Spark and PySpark"
      ],
      "metadata": {
        "id": "JPVFXzzhpQlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1\n",
        "# Set Up Spark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "firm8GrGpdEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
        "from pyspark.sql.functions import col, sum, count\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "e0Vbp2-bpqiv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"EcommerceRealTime\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "pKXwPlw4p16j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a directory for streaming data\n",
        "streaming_data_path = os.makedirs('/content/streaming_data', exist_ok=True)\n",
        "\n",
        "# Step 4: Define schema for order data\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"order_amount\", FloatType(), True),\n",
        "    StructField(\"order_date\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Step 5: Simulate initial streaming data with new values\n",
        "initial_order_data = [\n",
        "    (2001, 1, 3001, 2, 149.99, datetime.now()),  # Order 1\n",
        "    (2002, 2, 3002, 3, 199.99, datetime.now()),  # Order 2\n",
        "    (2003, 3, 3003, 1, 299.99, datetime.now()),  # Order 3\n",
        "    (2004, 4, 3001, 4, 399.99, datetime.now()),  # Order 4\n",
        "    (2005, 5, 3002, 2, 599.99, datetime.now()),  # Order 5\n",
        "]\n",
        "\n",
        "# Create a DataFrame from the list of initial orders\n",
        "df_orders = spark.createDataFrame(initial_order_data, schema)\n",
        "\n",
        "# Show the initial data\n",
        "print(\"Initial Orders:\")\n",
        "df_orders.show()\n",
        "\n",
        "# Step 6: Process Data Using PySpark\n",
        "# Group the data by product_id and calculate the total sales (sum of order_amount) per product\n",
        "product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "print(\"Initial Product Sales:\")\n",
        "product_sales.show()\n",
        "\n",
        "# Step 7: Real-Time Streaming Simulation\n",
        "# Simulating appending new data in real-time with updated values\n",
        "new_order_data = [\n",
        "    (1008, 2, 3004, 2, 249.99, datetime.now()),  # New Order 1\n",
        "    (1009, 4, 3001, 3, 359.95, datetime.now()),  # New Order 2\n",
        "    (1010, 5, 3002, 1, 599.99, datetime.now()),  # New Order 3\n",
        "    (1011, 1, 3003, 4, 149.99, datetime.now()),  # New Order 4\n",
        "    (1012, 3, 3004, 2, 389.50, datetime.now()),  # New Order 5\n",
        "]\n",
        "\n",
        "# Create a new DataFrame for the new batch of orders with the correct timestamp format\n",
        "new_df_orders = spark.createDataFrame(new_order_data, schema)\n",
        "\n",
        "# Append the new data to the original DataFrame\n",
        "df_orders = df_orders.union(new_df_orders)\n",
        "\n",
        "# Perform the same aggregation again with the updated data\n",
        "updated_product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the updated result\n",
        "print(\"Updated Product Sales After New Orders:\")\n",
        "updated_product_sales.show()\n",
        "\n",
        "# Step 8: Write the results to a CSV file\n",
        "updated_product_sales.write.csv(\"/content/product_sales.csv\", header=True, mode='overwrite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOl5yp7ZqH-Y",
        "outputId": "fe59ce43-f96a-451d-dbc7-4c9694ddbf7f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Orders:\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "|order_id|product_id|customer_id|quantity|order_amount|          order_date|\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "|    2001|         1|       3001|       2|      149.99|2024-09-27 09:45:...|\n",
            "|    2002|         2|       3002|       3|      199.99|2024-09-27 09:45:...|\n",
            "|    2003|         3|       3003|       1|      299.99|2024-09-27 09:45:...|\n",
            "|    2004|         4|       3001|       4|      399.99|2024-09-27 09:45:...|\n",
            "|    2005|         5|       3002|       2|      599.99|2024-09-27 09:45:...|\n",
            "+--------+----------+-----------+--------+------------+--------------------+\n",
            "\n",
            "Initial Product Sales:\n",
            "+----------+------------------+-----------+\n",
            "|product_id|       total_sales|order_count|\n",
            "+----------+------------------+-----------+\n",
            "|         1|149.99000549316406|          1|\n",
            "|         2|199.99000549316406|          1|\n",
            "|         3|  299.989990234375|          1|\n",
            "|         5|  599.989990234375|          1|\n",
            "|         4|  399.989990234375|          1|\n",
            "+----------+------------------+-----------+\n",
            "\n",
            "Updated Product Sales After New Orders:\n",
            "+----------+-----------------+-----------+\n",
            "|product_id|      total_sales|order_count|\n",
            "+----------+-----------------+-----------+\n",
            "|         1|299.9800109863281|          2|\n",
            "|         2|449.9800109863281|          2|\n",
            "|         3| 689.489990234375|          2|\n",
            "|         5| 1199.97998046875|          2|\n",
            "|         4|759.9400024414062|          2|\n",
            "+----------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
